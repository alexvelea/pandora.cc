\chapter{Glossary}
This chapter presents a short description of some popular terms which will be used throw out the paper, but it does not offer comparisons or advantages, but instead, it tries to paint a real picture that might escape the bounds of definitions found in the literature. The purpose is to familiarise readers with the terms and use cases so that it'll be easier to figure out what we're trying to achieve and what the limitations are.


\section{Arhitectures}
A software arhitecture defines the general structure of a software system. It embodies how the data is managed between, the relationships and requirements of the components. Chosing the right arhitecture can affect development in multiple ways, including reliability of the product, speed, development pace, scalability and upgradeability.

\subsection{Microservices}
Microservices, also known as microservice arhitecture is an arhitectural style that structures the core elements of the software into a collection of independent and scoped services. The term was introduced in 2013, being described as a granular approach to SOA(service oriented arhitecture).

While SOA breakes down functionality based on business logic, for example a simple \code{OrderService}, a microservice based design breaks down that functionality even further into \code{CustomerManagement}, \code{OrderManagement} and \code{OrderProcessing}.

Microservices are defined as grained services with precise business-capability, with a standardized interface and a lightweight protocol [Hassan et al. 2017]. Such microservices usually run in their own process, containerised, making them easily to deploy and upgrade [Alshuqayran et al. 2016]. The comunication between microservices is done using the network through a RESTfull API or RPC(Remote Procedure Call), making them language independent. This enables the use of multiple microservices in parallel behind a load balancer to achieve horizontal scaling, i.e, scaling up with the number of servers not the number of CPU cores.

\subsection{Nanoservices}

Nanoservices are commonly viewed as granular microservices. They share a big part of the design principles of microservices, the only difference being that nanoservices offer just one endpoint. If in a \code{OrderService} SOA application contains multiple microservices, such as \code{OrderManagement}, this can be split even further into smaller modular pieces, for example \code{PlaceOrder} and \code{CancelOrder} so that they can form a microservice when bundled together.

Nanoservice advocates try to provide a usecase-based definition, trying to focus on where a nanoservice should be used and what are the key differences in doing so when compared to a microservice. This enables a more robust definition which is not tied to the size or the number of APIs exposed. More precisely, a nanoservice is defined as a service which is deployable and most importantly reusable. One such example could be a service which provides files from various sources. This would be helpfull since all the other services share one unified API to retrieve files and they are agnostic on the location of these files. Some of them could be stored in the database while others could be in the cloud, in a storage server.

\section{Parallel computing}
Parallel computing refers to the act of running multiple computations simultaneously. Even though this was possible from the introduction of the first multi-core CPU, the trend has reached a golden age with the inability to scale up the frequency of cores, manufacturers opting to increase the number of cores/CPU instead of increasing the frequency of the actual cores. Not to be confused with distributed computing where one task is split among multiple computers inside the same network.

\subsection{Concurrency}
Concurrent computing, usually being mistaken for parallel computing, is a form of computing in which multiple tasks can have an overlapping execution time, but no 2 tasks will be running at the same time. This paradigm allows more accessible design since there is no need for locking shared resources between tasks while allowing multiple flows to execute seamlessly in parallel if the work to be done is lightweight. Usually, when talking about concurrency, tasks do not switch context mid-run, meaning that one task should finalize or yield (force pause) before the schedules choose another one. This can be seen in high-level languages that offer concurrency but not parallelisms, such as Python and JavaScript.

\subsection{Threads}
Threads, not to be confused with CPU threads, are theoretical code execution flows which belong to a process. A process can have multiple threads, each of them being can run in parallel. The OS(operating system) chooses when and which threads should run in one moment of time using the scheduler. When a thread switch is made, i.e., the OS chooses to run another thread, a context switch takes place, which adds an overhead for the operation. One main advantage of threads compared to having multiple processes running in parallel is memory sharing. While threads do not share the stack, they do share heap-allocated variables, making the transfer of information very convenient. Because of this, new atomic locking mechanisms need to be added, so synchronization of threads will be possible. These atomic operations can be seen as operations that happen one at a time.

\subsection{Promises and futures}
Promises and futures represent abstract ways to do parallel programming. These features can be implemented in the majority of languages, even if they do not support parallel programming per se. This is happening because they can be used as a means to retrieve information from the network, which can arrive at any time. A future can be viewed as a place holder for a variable that will be populated by another thread or action in the future. In C++, promises are a means of creating futures. The promise takes the action of setting the value of the promise for the generated child-future while other code-flows can wait for that to happen.

\subsection{Callbacks}
Callbacks represent another way to deal with asynchronous calls. Popularised by JavaScript, callbacks will execute some code when the result of an asynchronous call is determined. For simplicity, this concept is very similar to futures in the sense that it can be achieved by waiting on a future and executing the callback afterward while running the code between the initial function call and the future wait as another task on a different thread. Callbacks are generally used when the result is not influencing the state of the flow before the code, but either producing side-effects such as printing or changing UI or spawning additional calls on its own. An essential factor to consider is that while a future is being waited for, the thread doing so is waiting, wasting valuable CPU time. Based on this observation, callbacks tend to minimize wasted time.

\subsection{The actor model}
Being created in 1973 by Carl Hewitt, the actor model is a mathematical model that treats actors as the entities that carry out concurrent programming. An actor has a local scope state, the sharing of information being done throw sending messages. When an actor receives a message, it can carry out local operations, send other messages, create new actors, and modify its own state before sending out the reply. In this way, there is no need for lock-based synchronization, as in the case of thread-programming.

Nowadays, it's used mostly in proofs and in some functional programming languages since the coding mindset is more common than in imperative programming languages. Erlang was one of the first widely-used programming languages which adopted the actor model, introducing it's "let it crash" philosophy. This can happen because actors have limited scope, and in case of a fail, only that actor is affected by it, meaning it can be recreated with ease. With this being said, it's not uncommon to hear about applications that use the actor model, which has been running without any problem for more than 5 years. Erricson is reporting a $99.9999999\%$ reliability over the course of 20 years for it's ATM switch, being down for only %0.6% seconds.